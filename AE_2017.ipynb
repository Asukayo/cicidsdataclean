{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 方法1：使用SimHei字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "ab5d944a9b7931b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 配置参数\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {DEVICE}\")"
   ],
   "id": "977c63a59b6b84bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 自编码器模型定义\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # 编码器结构\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 解码器结构\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ],
   "id": "50f9528a54b5f66a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 实用函数：计算重构误差\n",
    "def compute_reconstruction_error(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_errors = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算每个样本的MSE重构误差\n",
    "            errors = torch.mean((outputs - inputs) ** 2, dim=1).cpu().numpy()\n",
    "            all_errors.extend(errors)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_errors), np.array(all_labels)"
   ],
   "id": "e619c421ff0a6071",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载数据函数\n",
    "def load_cicids_data(data_dir, first_day_name, second_day_name, window_size=10000, step_size=1000):\n",
    "    \"\"\"\n",
    "    加载CICIDS数据集的两天数据\n",
    "    \n",
    "    参数:\n",
    "    - data_dir: 包含处理后窗口数据的目录\n",
    "    - first_day_name: 第一天的数据目录名\n",
    "    - second_day_name: 第二天的数据目录名\n",
    "    \n",
    "    返回:\n",
    "    - train_data: 第一天的窗口特征数据\n",
    "    - train_labels: 第一天的窗口标签（0=正常，1=异常）\n",
    "    - test_data: 第二天的窗口特征数据\n",
    "    - test_labels: 第二天的窗口标签（0=正常，1=异常）\n",
    "    - feature_names: 特征名称列表\n",
    "    \"\"\"\n",
    "    # 加载第一天数据\n",
    "    day1_dir = os.path.join(data_dir, first_day_name)\n",
    "    X_train = np.load(os.path.join(day1_dir, f'X_windows_w{window_size}_s{step_size}.npy'))\n",
    "    \n",
    "    # 加载元数据以获取特征名称和窗口标签信息\n",
    "    with open(os.path.join(day1_dir, f'metadata_w{window_size}_s{step_size}.pkl'), 'rb') as f:\n",
    "        day1_metadata = pickle.load(f)\n",
    "    \n",
    "    # 获取特征名称\n",
    "    feature_names = day1_metadata['feature_names']\n",
    "    \n",
    "    # 从元数据中提取窗口级别的标签（0=正常，1=异常）\n",
    "    train_labels = np.array([m['is_malicious'] for m in day1_metadata['window_metadata']])\n",
    "    \n",
    "    # 加载第二天数据\n",
    "    day2_dir = os.path.join(data_dir, second_day_name)\n",
    "    X_test = np.load(os.path.join(day2_dir, f'X_windows_w{window_size}_s{step_size}.npy'))\n",
    "    \n",
    "    with open(os.path.join(day2_dir, f'metadata_w{window_size}_s{step_size}.pkl'), 'rb') as f:\n",
    "        day2_metadata = pickle.load(f)\n",
    "    \n",
    "    # 从元数据中提取窗口级别的标签\n",
    "    test_labels = np.array([m['is_malicious'] for m in day2_metadata['window_metadata']])\n",
    "    \n",
    "    print(f\"加载完成: {first_day_name} 包含 {len(train_labels)} 个窗口, {second_day_name} 包含 {len(test_labels)} 个窗口\")\n",
    "    print(f\"特征维度: {X_train.shape}\")\n",
    "    \n",
    "    return X_train, train_labels, X_test, test_labels, feature_names\n"
   ],
   "id": "f75f8075ce28c13b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 预处理数据函数\n",
    "def preprocess_data(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    对窗口数据进行预处理，包括重塑和标准化\n",
    "    \n",
    "    参数:\n",
    "    - X_train: 训练窗口数据 [n_windows, window_size, n_features]\n",
    "    - y_train: 训练标签\n",
    "    - X_test: 测试窗口数据 [n_windows, window_size, n_features]\n",
    "    - y_test: 测试标签\n",
    "    \n",
    "    返回:\n",
    "    - 预处理后的数据\n",
    "    \"\"\"\n",
    "    n_train_windows, window_size, n_features = X_train.shape\n",
    "    n_test_windows = X_test.shape[0]\n",
    "    \n",
    "    # 计算每个窗口的特征均值，简化为[n_windows, n_features]形状\n",
    "    # 这是一个简单的方法来处理窗口中的时间序列数据\n",
    "    X_train_mean = np.mean(X_train, axis=1)\n",
    "    X_test_mean = np.mean(X_test, axis=1)\n",
    "    \n",
    "    # 标准化\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_mean)\n",
    "    X_test_scaled = scaler.transform(X_test_mean)\n",
    "    \n",
    "    print(f\"预处理后形状: 训练集 {X_train_scaled.shape}, 测试集 {X_test_scaled.shape}\")\n",
    "    \n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test, scaler"
   ],
   "id": "fe4e617a852f25cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 训练函数\n",
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    训练自编码器模型\n",
    "    \n",
    "    参数:\n",
    "    - model: 自编码器模型\n",
    "    - train_loader: 训练数据加载器\n",
    "    - val_loader: 验证数据加载器\n",
    "    - epochs: 训练轮数\n",
    "    - learning_rate: 学习率\n",
    "    - device: 训练设备\n",
    "    \n",
    "    返回:\n",
    "    - 训练历史\n",
    "    \"\"\"\n",
    "    # 损失函数为MSE均方损失函数\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    # 获取只包含正常样本的训练加载器\n",
    "    benign_train_loader = train_loader\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for data, _ in tqdm(benign_train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(benign_train_loader)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, _ in val_loader:\n",
    "                data = data.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, data)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 记录历史\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # 打印进度\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "    return history\n",
    "\n"
   ],
   "id": "910106fb2ddd3479",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 评估函数\n",
    "def evaluate_model(model, test_loader, y_test, device):\n",
    "    \"\"\"\n",
    "    评估模型性能\n",
    "    \n",
    "    参数:\n",
    "    - model: 训练好的自编码器模型\n",
    "    - test_loader: 测试数据加载器\n",
    "    - y_test: 测试标签\n",
    "    - device: 设备\n",
    "    \"\"\"\n",
    "    # 计算重构误差\n",
    "    reconstruction_errors, _ = compute_reconstruction_error(model, test_loader, device)\n",
    "    \n",
    "    # 使用重构误差作为异常分数\n",
    "    y_score = reconstruction_errors\n",
    "    \n",
    "    # 计算ROC和PR曲线\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    pr_auc = average_precision_score(y_test, y_score)\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, 'b', label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # 绘制PR曲线\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, 'g', label=f'AP = {pr_auc:.2f}')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation_curves.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC Score: {pr_auc:.4f}\")\n",
    "    \n",
    "    # 找出最佳阈值（使用F1分数）\n",
    "    thresholds = np.linspace(min(y_score), max(y_score), 100)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_score >= threshold).astype(int)\n",
    "        TP = np.sum((y_pred == 1) & (y_test == 1))\n",
    "        FP = np.sum((y_pred == 1) & (y_test == 0))\n",
    "        FN = np.sum((y_pred == 0) & (y_test == 1))\n",
    "        \n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # 使用最佳阈值进行预测\n",
    "    y_pred = (y_score >= best_threshold).astype(int)\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    TP = np.sum((y_pred == 1) & (y_test == 1))\n",
    "    TN = np.sum((y_pred == 0) & (y_test == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_test == 0))\n",
    "    FN = np.sum((y_pred == 0) & (y_test == 1))\n",
    "    \n",
    "    # 计算各种评估指标\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    print(f\"\\n最佳阈值: {best_threshold:.6f}\")\n",
    "    print(f\"准确率: {accuracy:.4f}\")\n",
    "    print(f\"精确率: {precision:.4f}\")\n",
    "    print(f\"召回率: {recall:.4f}\")\n",
    "    print(f\"F1分数: {f1:.4f}\")\n",
    "    \n",
    "    # 分析错误分类的窗口\n",
    "    error_indices = np.where(y_pred != y_test)[0]\n",
    "    print(f\"\\n错误分类的窗口数量: {len(error_indices)}\")\n",
    "    \n",
    "    if len(error_indices) > 0:\n",
    "        # 显示一些错误分类窗口的重构误差\n",
    "        print(\"\\n错误分类窗口的重构误差:\")\n",
    "        for i in range(min(5, len(error_indices))):\n",
    "            idx = error_indices[i]\n",
    "            print(f\"窗口 {idx}: 真实标签 = {y_test[idx]}, 预测标签 = {y_pred[idx]}, 重构误差 = {y_score[idx]:.6f}\")\n",
    "    \n",
    "    # 绘制重构误差分布\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    benign_errors = y_score[y_test == 0]\n",
    "    malicious_errors = y_score[y_test == 1]\n",
    "    \n",
    "    plt.hist(benign_errors, bins=50, alpha=0.7, label='正常', density=True)\n",
    "    plt.hist(malicious_errors, bins=50, alpha=0.7, label='异常', density=True)\n",
    "    plt.axvline(best_threshold, color='r', linestyle='--', label=f'阈值 = {best_threshold:.4f}')\n",
    "    \n",
    "    plt.xlabel('重构误差')\n",
    "    plt.ylabel('密度')\n",
    "    plt.title('重构误差分布')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('reconstruction_error_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return y_score, best_threshold\n"
   ],
   "id": "ca1e87adf3618c92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 数据目录设置\n",
    "data_dir = r\"cicids2017/flow_windows_w10000_s1000\"  # 修改为你的数据目录\n",
    "first_day = \"Monday-WorkingHours\"\n",
    "second_day = \"Tuesday-WorkingHours\"\n",
    "window_size = 10000\n",
    "step_size = 1000"
   ],
   "id": "dc5da1ece40c5664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载数据\n",
    "X_train, y_train, X_test, y_test, feature_names = load_cicids_data(\n",
    "    data_dir, first_day, second_day, \n",
    "    window_size=window_size, step_size=step_size\n",
    ")\n",
    "    \n",
    "# 数据预处理\n",
    "X_train_processed, y_train, X_test_processed, y_test, scaler = preprocess_data(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")"
   ],
   "id": "70efa0c66e9fb72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 分离正常样本进行训练 (仅使用正常样本进行训练)\n",
    "benign_indices = np.where(y_train == 0)[0]\n",
    "X_train_benign = X_train_processed[benign_indices]\n",
    "    "
   ],
   "id": "fefa821f8eefc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 准备数据加载器\n",
    "# 训练集（只使用正常样本）\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train_benign), \n",
    "                                  torch.FloatTensor(np.zeros(len(X_train_benign))))\n",
    "    "
   ],
   "id": "bde3f497654dfe73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 将训练集拆分为训练和验证集\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ],
   "id": "2cd1bc11ac4377ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 测试集（包含正常和异常样本）\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test_processed), \n",
    "                                torch.FloatTensor(y_test))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ],
   "id": "2cea59a341f58a91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 初始化模型\n",
    "input_dim = X_train_processed.shape[1]\n",
    "latent_dim = 16\n",
    "model = AutoEncoder(input_dim=input_dim, latent_dim=latent_dim).to(DEVICE)\n",
    "print(model)\n",
    "    \n",
    "# 训练模型\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=DEVICE\n",
    ")\n",
    "    \n",
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_loss'], label='训练损失')\n",
    "plt.plot(history['val_loss'], label='验证损失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE损失')\n",
    "plt.title('训练和验证损失')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()\n",
    "    \n",
    "# 评估模型\n",
    "y_score, threshold = evaluate_model(model, test_loader, y_test, DEVICE)\n",
    "    \n",
    "# 保存模型和阈值\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_dim': input_dim,\n",
    "    'latent_dim': latent_dim,\n",
    "    'threshold': threshold,\n",
    "    'scaler': scaler\n",
    "}, 'cicids_autoencoder.pth')\n",
    "    \n",
    "print(\"模型已保存至 'cicids_autoencoder.pth'\")"
   ],
   "id": "a93a17db96b359a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T06:15:51.495215Z",
     "start_time": "2025-04-14T06:15:45.614401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # 数据目录设置\n",
    "    data_dir = \"cicids2017/flow_windows_w10000_s1000\"  # 修改为你的数据目录\n",
    "    first_day = \"Monday-WorkingHours\"\n",
    "    second_day = \"Tuesday-WorkingHours\"\n",
    "    window_size = 1000\n",
    "    step_size = 100\n",
    "    \n",
    "    # 加载数据\n",
    "    X_train, y_train, X_test, y_test, feature_names = load_cicids_data(\n",
    "        data_dir, first_day, second_day, \n",
    "        window_size=window_size, step_size=step_size\n",
    "    )\n",
    "    \n",
    "    # 数据预处理\n",
    "    # X_train_processed, y_train, X_test_processed, y_test, scaler = preprocess_data(\n",
    "    #     X_train, y_train, X_test, y_test\n",
    "    # )\n",
    "    \n",
    "    # 分离正常样本进行训练 (仅使用正常样本进行训练)\n",
    "    benign_indices = np.where(y_train == 0)[0]\n",
    "    X_train_benign = X_train[benign_indices]\n",
    "    \n",
    "    # 准备数据加载器\n",
    "    # 训练集（只使用正常样本）\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_benign), \n",
    "                                  torch.FloatTensor(np.zeros(len(X_train_benign))))\n",
    "    \n",
    "    # 将训练集拆分为训练和验证集\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # 测试集（包含正常和异常样本）\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test), \n",
    "                                torch.FloatTensor(y_test))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # 初始化模型\n",
    "    input_dim = X_train.shape[1]\n",
    "    latent_dim = 16\n",
    "    model = AutoEncoder(input_dim=input_dim, latent_dim=latent_dim).to(DEVICE)\n",
    "    print(model)\n",
    "    \n",
    "    # 训练模型\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train_loss'], label='训练损失')\n",
    "    plt.plot(history['val_loss'], label='验证损失')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE损失')\n",
    "    plt.title('训练和验证损失')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # 评估模型\n",
    "    y_score, threshold = evaluate_model(model, test_loader, y_test, DEVICE)\n",
    "    \n",
    "    # 保存模型和阈值\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'input_dim': input_dim,\n",
    "        'latent_dim': latent_dim,\n",
    "        'threshold': threshold,\n",
    "        'scaler': scaler\n",
    "    }, 'cicids_autoencoder.pth')\n",
    "    \n",
    "    print(\"模型已保存至 'cicids_autoencoder.pth'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "80a6478250f4c61b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载完成: Monday-WorkingHours 包含 5017 个窗口, Tuesday-WorkingHours 包含 4207 个窗口\n",
      "特征维度: (5017, 1000, 68)\n",
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (12): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "    (7): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.2, inplace=False)\n",
      "    (11): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64000x68 and 1000x256)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18604\\1893996215.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"__main__\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 86\u001B[1;33m     \u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18604\\1893996215.py\u001B[0m in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     53\u001B[0m         \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m         \u001B[0mlearning_rate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mLEARNING_RATE\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 55\u001B[1;33m         \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     56\u001B[0m     )\n\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18604\\3735954902.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, val_loader, epochs, learning_rate, device)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m             \u001B[1;31m# 前向传播\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 39\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     40\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\SoftWare\\anaconda3\\envs\\2017clean\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1195\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18604\\4243635623.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 47\u001B[1;33m         \u001B[0mencoded\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     48\u001B[0m         \u001B[0mdecoded\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoded\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mdecoded\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\SoftWare\\anaconda3\\envs\\2017clean\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1195\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\SoftWare\\anaconda3\\envs\\2017clean\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 204\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    205\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\SoftWare\\anaconda3\\envs\\2017clean\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1195\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\SoftWare\\anaconda3\\envs\\2017clean\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 114\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    115\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (64000x68 and 1000x256)"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
