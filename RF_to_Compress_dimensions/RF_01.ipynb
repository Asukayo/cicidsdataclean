{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-04T05:16:38.650629Z",
     "start_time": "2025-06-04T05:16:38.643190Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:00:36.881946Z",
     "start_time": "2025-06-04T02:00:36.874910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 强制使用无GUI后端\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# 使用DejaVu Sans字体（大多数Linux系统都有）\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "e9c67cac658bcf57",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T05:12:51.363648Z",
     "start_time": "2025-06-04T05:12:51.347501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess data\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(df['Label'].value_counts())\n",
    "\n",
    "    # Binary classification (Normal vs Anomaly)\n",
    "    df['binary_label'] = df['Label'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
    "\n",
    "    # Get feature columns\n",
    "    exclude_cols = ['Timestamp', 'Label', 'binary_label']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    # Handle infinite and missing values\n",
    "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    missing_counts = df[feature_cols].isnull().sum()\n",
    "    if missing_counts.sum() > 0:\n",
    "        print(f\"\\nFound missing values: {missing_counts.sum()}\")\n",
    "        df[feature_cols] = df[feature_cols].fillna(0)\n",
    "\n",
    "    print(f\"\\nNumber of features: {len(feature_cols)}\")\n",
    "    \n",
    "    # use standard normalization\n",
    "    print(\"Applying scaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "    #check success or not\n",
    "    normalized_stats = df[feature_cols].describe()\n",
    "    mean_check = abs(normalized_stats.loc['mean'].mean()) < 1e-10\n",
    "    std_check = abs(normalized_stats.loc['std'].mean()-1.0) < 1e-10\n",
    "    print(f\"\\nNormalization check - mean: {mean_check}, std: {std_check}\")\n",
    "    return df, feature_cols"
   ],
   "id": "c6a736e7861ba3ee",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:00:40.483401Z",
     "start_time": "2025-06-04T02:00:40.473228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_feature_importance(X, y, n_estimators=100, max_samples=50000):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using Random Forest\n",
    "    \"\"\"\n",
    "    print(f\"\\nCalculating feature importance...\")\n",
    "    print(f\"Parameters: n_estimators={n_estimators}\")\n",
    "\n",
    "    if len(X) > max_samples:\n",
    "        print(f\"Large dataset ({len(X)} samples), sampling {max_samples}...\")\n",
    "        _, X_sample, _, y_sample = train_test_split(\n",
    "            X, y, test_size=max_samples / len(X),\n",
    "            random_state=42, stratify=y\n",
    "        )\n",
    "    else:\n",
    "        X_sample = X\n",
    "        y_sample = y\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_classifier.fit(X_sample, y_sample)\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "    return feature_importances"
   ],
   "id": "3f5f3e7f74f29124",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:00:42.392928Z",
     "start_time": "2025-06-04T02:00:42.383509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_and_visualize_importance(feature_cols, feature_importances, top_n=30):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance\n",
    "    \"\"\"\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': feature_importances\n",
    "    })\n",
    "\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "    importance_df['cumulative_importance'] = importance_df['importance'].cumsum()\n",
    "\n",
    "    print(f\"\\nTop {top_n} most important features:\")\n",
    "    print(\"-\" * 70)\n",
    "    for idx, row in importance_df.head(top_n).iterrows():\n",
    "        print(f\"{idx + 1:3d}. {row['feature']:45s} {row['importance']:.6f}\")\n",
    "\n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "    # Plot 1: Top N features importance bar chart\n",
    "    top_features = importance_df.head(top_n)\n",
    "    bars = ax1.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "    ax1.set_yticks(range(len(top_features)))\n",
    "    ax1.set_yticklabels(top_features['feature'], fontsize=8)\n",
    "    ax1.set_xlabel('Feature Importance', fontsize=12)\n",
    "    ax1.set_title(f'Top {top_n} Most Important Features', fontsize=14, fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', ha='left', va='center', fontsize=7)\n",
    "\n",
    "    # Plot 2: Cumulative importance curve\n",
    "    ax2.plot(range(1, len(importance_df) + 1), importance_df['cumulative_importance'], \n",
    "             'b-', linewidth=2, label='Cumulative Importance')\n",
    "    ax2.axhline(y=0.95, color='r', linestyle='--', linewidth=2, label='95% Threshold')\n",
    "    ax2.set_xlabel('Number of Features', fontsize=12)\n",
    "    ax2.set_ylabel('Cumulative Importance', fontsize=12)\n",
    "    ax2.set_title('Feature Cumulative Importance Curve', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Mark 95% point\n",
    "    n_features_95 = (importance_df['cumulative_importance'] >= 0.95).idxmax() + 1\n",
    "    ax2.axvline(x=n_features_95, color='g', linestyle='--', linewidth=2,\n",
    "                label=f'{n_features_95} features reach 95%')\n",
    "    ax2.legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('feature_importance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig('feature_importance_analysis.pdf', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Plots saved as 'feature_importance_analysis.png' and '.pdf'\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\n✓ {n_features_95} features needed to reach 95% cumulative importance\")\n",
    "    return importance_df"
   ],
   "id": "92d5fe8ad4422778",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1f3cc36d731649d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:00:45.159143Z",
     "start_time": "2025-06-04T02:00:45.141666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_distribution_plot(importance_df):\n",
    "    \"\"\"\n",
    "    Create feature importance distribution plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Main histogram\n",
    "    n, bins, patches = plt.hist(importance_df['importance'], bins=50, \n",
    "                               edgecolor='black', alpha=0.7, color='lightblue')\n",
    "    \n",
    "    # Color bars based on importance\n",
    "    for i, patch in enumerate(patches):\n",
    "        if bins[i] > importance_df['importance'].quantile(0.9):\n",
    "            patch.set_facecolor('red')\n",
    "        elif bins[i] > importance_df['importance'].quantile(0.7):\n",
    "            patch.set_facecolor('orange')\n",
    "    \n",
    "    plt.xlabel('Feature Importance', fontsize=12)\n",
    "    plt.ylabel('Number of Features', fontsize=12)\n",
    "    plt.title('Feature Importance Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add statistics lines\n",
    "    mean_val = importance_df['importance'].mean()\n",
    "    median_val = importance_df['importance'].median()\n",
    "    \n",
    "    plt.axvline(x=mean_val, color='r', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {mean_val:.6f}')\n",
    "    plt.axvline(x=median_val, color='g', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {median_val:.6f}')\n",
    "    \n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text box with statistics\n",
    "    stats_text = f\"\"\"Statistics:\n",
    "    Mean: {mean_val:.6f}\n",
    "    Median: {median_val:.6f}\n",
    "    Std: {importance_df['importance'].std():.6f}\n",
    "    Max: {importance_df['importance'].max():.6f}\n",
    "    Min: {importance_df['importance'].min():.6f}\"\"\"\n",
    "    \n",
    "    plt.text(0.7, 0.7, stats_text, transform=plt.gca().transAxes,\n",
    "             fontsize=9, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig('feature_importance_distribution.pdf', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Distribution plots saved as 'feature_importance_distribution.png' and '.pdf'\")\n",
    "    plt.close()\n"
   ],
   "id": "3b5e25fedd5025a0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:00:47.648935Z",
     "start_time": "2025-06-04T02:00:47.640249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results(importance_df, output_path='feature_importance_results.csv'):\n",
    "    \"\"\"\n",
    "    Save feature importance results\n",
    "    \"\"\"\n",
    "    importance_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✓ Feature importance results saved to: {output_path}\")\n",
    "\n",
    "    # Save top features for 95% cumulative importance\n",
    "    n_features_95 = (importance_df['cumulative_importance'] >= 0.95).idxmax() + 1\n",
    "    important_features = importance_df.head(n_features_95)['feature'].tolist()\n",
    "\n",
    "    with open('important_features_95.txt', 'w') as f:\n",
    "        for feature in important_features:\n",
    "            f.write(f\"{feature}\\n\")\n",
    "\n",
    "    print(f\"✓ Top {n_features_95} important features saved to: important_features_95.txt\")\n",
    "    \n",
    "    # Save different threshold versions\n",
    "    thresholds = [0.8, 0.9, 0.95, 0.99]\n",
    "    for threshold in thresholds:\n",
    "        n_features = (importance_df['cumulative_importance'] >= threshold).idxmax() + 1\n",
    "        features = importance_df.head(n_features)['feature'].tolist()\n",
    "        \n",
    "        filename = f'important_features_{int(threshold*100)}.txt'\n",
    "        with open(filename, 'w') as f:\n",
    "            for feature in features:\n",
    "                f.write(f\"{feature}\\n\")\n",
    "        print(f\"✓ Top {n_features} features ({threshold*100}%) saved to: {filename}\")"
   ],
   "id": "edcd2d8d6e93c5fb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5b021aed2beb060a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:00:49.730802Z",
     "start_time": "2025-06-04T02:00:49.716208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_detailed_analysis(importance_df):\n",
    "    \"\"\"\n",
    "    Print detailed analysis results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    print(f\"  Total features: {len(importance_df)}\")\n",
    "    print(f\"  Mean importance: {importance_df['importance'].mean():.6f}\")\n",
    "    print(f\"  Median importance: {importance_df['importance'].median():.6f}\")\n",
    "    print(f\"  Standard deviation: {importance_df['importance'].std():.6f}\")\n",
    "    print(f\"  Max importance: {importance_df['importance'].max():.6f}\")\n",
    "    print(f\"  Min importance: {importance_df['importance'].min():.6f}\")\n",
    "    \n",
    "    # Features needed for different thresholds\n",
    "    print(f\"\\nFeatures needed for different cumulative importance thresholds:\")\n",
    "    for threshold in [0.5, 0.8, 0.9, 0.95, 0.99]:\n",
    "        n_features = (importance_df['cumulative_importance'] >= threshold).idxmax() + 1\n",
    "        percentage = (n_features / len(importance_df)) * 100\n",
    "        print(f\"  {threshold*100:2.0f}%: {n_features:4d} features ({percentage:5.1f}% of total)\")\n",
    "    \n",
    "    # Low importance features\n",
    "    low_thresholds = [0.0001, 0.001, 0.01]\n",
    "    for threshold in low_thresholds:\n",
    "        low_features = importance_df[importance_df['importance'] < threshold]\n",
    "        percentage = (len(low_features) / len(importance_df)) * 100\n",
    "        print(f\"\\nFeatures with importance < {threshold}: {len(low_features)} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if len(low_features) > 0 and threshold == 0.0001:\n",
    "            print(\"  These features could be candidates for removal:\")\n",
    "            for _, row in low_features.head(5).iterrows():\n",
    "                print(f\"    - {row['feature']}: {row['importance']:.8f}\")\n",
    "            if len(low_features) > 5:\n",
    "                print(f\"    ... and {len(low_features) - 5} more\")"
   ],
   "id": "80f7a23e7bdb720",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4853d5a8ede47c94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T05:31:48.240598Z",
     "start_time": "2025-06-04T05:25:51.310546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "# Configuration\n",
    "data_path = \"../cicids2017/clean/all_data.parquet\"\n",
    "n_estimators = 50000  # Reduced for faster execution\n",
    "max_samples = 100000\n",
    "top_n_display = 30\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CICIDS2017 FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # 1. Load and preprocess data\n",
    "    df, feature_cols = load_and_preprocess_data(data_path)\n",
    "\n",
    "    # 2. Prepare feature matrix and labels\n",
    "    X = df[feature_cols].values\n",
    "    y = df['binary_label'].values\n",
    "\n",
    "    print(f\"\\n📊 Dataset Summary:\")\n",
    "    print(f\"   Feature matrix shape: {X.shape}\")\n",
    "    print(f\"   Normal samples: {(y == 0).sum():,} ({(y == 0).sum() / len(y) * 100:.2f}%)\")\n",
    "    print(f\"   Anomalous samples: {(y == 1).sum():,} ({(y == 1).sum() / len(y) * 100:.2f}%)\")\n",
    "\n",
    "    # 3. Calculate feature importance\n",
    "    feature_importances = calculate_feature_importance(\n",
    "        X, y, n_estimators=n_estimators, max_samples=max_samples\n",
    "    )\n",
    "\n",
    "    # 4. Analyze and visualize results\n",
    "    importance_df = analyze_and_visualize_importance(\n",
    "        feature_cols, feature_importances, top_n=top_n_display\n",
    "    )\n",
    "\n",
    "    # 5. Create distribution plot\n",
    "    create_distribution_plot(importance_df)\n",
    "\n",
    "    # 6. Save results\n",
    "    save_results(importance_df)\n",
    "\n",
    "    # 7. Print detailed analysis\n",
    "    print_detailed_analysis(importance_df)\n",
    "\n",
    "    print(f\"\\n🎉 Analysis completed successfully!\")\n",
    "    print(f\"📁 Check the following output files:\")\n",
    "    print(f\"   - feature_importance_analysis.png/pdf\")\n",
    "    print(f\"   - feature_importance_distribution.png/pdf\")\n",
    "    print(f\"   - feature_importance_results.csv\")\n",
    "    print(f\"   - important_features_*.txt\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "id": "42f60d2325ea6263",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CICIDS2017 FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "Loading data...\n",
      "Data shape: (2497441, 70)\n",
      "\n",
      "Label distribution:\n",
      "Benign                        2071709\n",
      "DoS Hulk                       172837\n",
      "DDoS                           128014\n",
      "PortScan                        90694\n",
      "DoS GoldenEye                   10286\n",
      "FTP-Patator                      5931\n",
      "DoS slowloris                    5385\n",
      "DoS Slowhttptest                 5228\n",
      "SSH-Patator                      3219\n",
      "Bot                              1948\n",
      "Web Attack  Brute Force         1470\n",
      "Web Attack  XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack  Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Number of features: 68\n",
      "Applying scaler...\n",
      "\n",
      "Normalization check - mean: True, std: False\n",
      "\n",
      "📊 Dataset Summary:\n",
      "   Feature matrix shape: (2497441, 68)\n",
      "   Normal samples: 2,071,709 (82.95%)\n",
      "   Anomalous samples: 425,732 (17.05%)\n",
      "\n",
      "Calculating feature importance...\n",
      "Parameters: n_estimators=50000\n",
      "Large dataset (2497441 samples), sampling 100000...\n",
      "Training Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 3986 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 4936 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 5986 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 7136 tasks      | elapsed:   48.6s\n",
      "[Parallel(n_jobs=-1)]: Done 8386 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=-1)]: Done 9736 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11186 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 12736 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 14386 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16136 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 17986 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 19936 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 21986 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 24136 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 26386 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 28736 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 31186 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 33736 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 36386 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 39136 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 41986 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 44936 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 47986 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 50000 out of 50000 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 30 most important features:\n",
      "----------------------------------------------------------------------\n",
      "  1. Bwd Packet Length Std                         0.081605\n",
      "  2. Packet Length Variance                        0.073981\n",
      "  3. Packet Length Std                             0.073657\n",
      "  4. Avg Bwd Segment Size                          0.056573\n",
      "  5. Bwd Packet Length Mean                        0.056257\n",
      "  6. Avg Packet Size                               0.044413\n",
      "  7. Bwd Packet Length Max                         0.041143\n",
      "  8. Packet Length Max                             0.038322\n",
      "  9. Packet Length Mean                            0.033564\n",
      " 10. Subflow Bwd Bytes                             0.027837\n",
      " 11. Bwd Packets Length Total                      0.027642\n",
      " 12. Destination Port                              0.025845\n",
      " 13. Fwd Packets Length Total                      0.021124\n",
      " 14. Subflow Fwd Packets                           0.020820\n",
      " 15. Subflow Fwd Bytes                             0.020737\n",
      " 16. Total Fwd Packets                             0.020479\n",
      " 17. Fwd Header Length                             0.015766\n",
      " 18. Fwd Seg Size Min                              0.015402\n",
      " 19. Fwd Packet Length Max                         0.015182\n",
      " 20. Fwd IAT Std                                   0.014814\n",
      " 21. Bwd Header Length                             0.014465\n",
      " 22. Fwd Act Data Packets                          0.012825\n",
      " 23. PSH Flag Count                                0.012699\n",
      " 24. Flow IAT Max                                  0.012166\n",
      " 25. Avg Fwd Segment Size                          0.012089\n",
      " 26. Fwd Packet Length Mean                        0.011994\n",
      " 27. Fwd IAT Max                                   0.011771\n",
      " 28. Init Bwd Win Bytes                            0.011499\n",
      " 29. ACK Flag Count                                0.010133\n",
      " 30. Init Fwd Win Bytes                            0.009818\n",
      "✓ Plots saved as 'feature_importance_analysis.png' and '.pdf'\n",
      "\n",
      "✓ 44 features needed to reach 95% cumulative importance\n",
      "✓ Distribution plots saved as 'feature_importance_distribution.png' and '.pdf'\n",
      "\n",
      "✓ Feature importance results saved to: feature_importance_results.csv\n",
      "✓ Top 44 important features saved to: important_features_95.txt\n",
      "✓ Top 26 features (80.0%) saved to: important_features_80.txt\n",
      "✓ Top 36 features (90.0%) saved to: important_features_90.txt\n",
      "✓ Top 44 features (95.0%) saved to: important_features_95.txt\n",
      "✓ Top 57 features (99.0%) saved to: important_features_99.txt\n",
      "\n",
      "================================================================================\n",
      "DETAILED FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Basic Statistics:\n",
      "  Total features: 68\n",
      "  Mean importance: 0.014706\n",
      "  Median importance: 0.009290\n",
      "  Standard deviation: 0.018448\n",
      "  Max importance: 0.081605\n",
      "  Min importance: 0.000000\n",
      "\n",
      "Features needed for different cumulative importance thresholds:\n",
      "  50%:   10 features ( 14.7% of total)\n",
      "  80%:   26 features ( 38.2% of total)\n",
      "  90%:   36 features ( 52.9% of total)\n",
      "  95%:   44 features ( 64.7% of total)\n",
      "  99%:   57 features ( 83.8% of total)\n",
      "\n",
      "Features with importance < 0.0001: 2 (2.9%)\n",
      "  These features could be candidates for removal:\n",
      "    - RST Flag Count: 0.00000002\n",
      "    - ECE Flag Count: 0.00000001\n",
      "\n",
      "Features with importance < 0.001: 6 (8.8%)\n",
      "\n",
      "Features with importance < 0.01: 39 (57.4%)\n",
      "\n",
      "🎉 Analysis completed successfully!\n",
      "📁 Check the following output files:\n",
      "   - feature_importance_analysis.png/pdf\n",
      "   - feature_importance_distribution.png/pdf\n",
      "   - feature_importance_results.csv\n",
      "   - important_features_*.txt\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
